{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60e2240-e5ce-4f7b-b436-b1126bbc6593",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557d517-d46f-4aa0-9eaf-be6f58b1a79d",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to use the Keras library to build models for classificaiton problems. We will use the popular MNIST dataset, a dataset of images. \n",
    "\n",
    "The MNIST database, short for Modified National Institute of Standards and Technology database, is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.\n",
    "    \n",
    "The MNIST database contains 60,000 training images and 10,000 testing images of digits written by high school students and employees of the United States Census Bureau.\n",
    "\n",
    "Also, this way, we will get to compare how Artificial neural networks compare to convolutional neural networks, that we will build in the next notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b909ae-4a8f-4bde-a66b-090afd8151eb",
   "metadata": {},
   "source": [
    "## Objectives for this Notebook    \n",
    "* Use the MNIST database for training various image processing systems\n",
    "* Build a neural network\n",
    "* Train and test the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aebb57-a565-45b5-9470-803d2f5eefd2",
   "metadata": {},
   "source": [
    "Let's start by installing Keras and other necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bee01a-08f2-448a-9c9a-ad111912c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow_cpu==2.18.0\n",
    "%pip install matplotlib==3.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0382f2-f135-4870-9d71-212e59c9f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b413e7b-7155-4227-8c72-9b22fc7ffd49",
   "metadata": {},
   "source": [
    "## Import Keras and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4895aa9-8bcb-46e6-a79c-a7a881c670ba",
   "metadata": {},
   "source": [
    "### Import the libraries. \n",
    "There might be some warning messages related to floating point round off errors and lack of GPU and other compiler related options. You can ignore these warnings and proceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c0174a-5eef-4c5f-8a9c-54c150f90ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe0e17-5d41-4dfb-8c57-b86293309403",
   "metadata": {},
   "source": [
    "Loading the MNIST dataset from the Keras library. The dataset is readily divided into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb035b3-f6b0-455c-9c43-e7445d3a42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# read the data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca734f-7ccb-4ec6-9dcc-47462d8db865",
   "metadata": {},
   "source": [
    "Let's confirm the number of images in each set. According to the dataset's documentation, we should have 60000 images in X_train and 10000 images in the X_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c93f66-dac7-4937-ae8f-2f346e6dd258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9c1ec-94a1-4545-9205-133bd8476ee0",
   "metadata": {},
   "source": [
    "The first number in the output tuple is the number of images, and the other two numbers are the size of the images in datset. So, each image is 28 pixels by 28 pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b2825-42a0-4052-b0d5-5ae02dc5e859",
   "metadata": {},
   "source": [
    "Let's visualize the first image in the training set using Matplotlib's scripting layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755619b2-64e3-416d-922b-8e6b8526d05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1afd3e81730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f703ea-874d-47dd-9d7a-20c78cc13b36",
   "metadata": {},
   "source": [
    "With conventional neural networks, we cannot feed in the image as input as is. So we need to flatten the images into one-dimensional vectors, each of size 1 x (28 x 28) = 1 x 784.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9a137c1-3554-4027-86f0-b1409ba810f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten images into one-dimensional vector\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2] # find size of one-dimensional vector\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32') # flatten training images\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32') # flatten test images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da865046-1d25-4855-bb8d-3b6bfb15343a",
   "metadata": {},
   "source": [
    "Since pixel values can range from 0 to 255, let's normalize the vectors to be between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b71bef-dd01-4e76-914e-f97e280b3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba8319-79d1-4079-9afd-047692d9f3e4",
   "metadata": {},
   "source": [
    "Finally, before we start building our model, remember that for classification we need to divide our target variable into categories. We use the to_categorical function from the Keras Utilities package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "416635d1-1051-4b66-9acb-8dd5b8e68290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# one hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ce88d-0157-43f2-b64f-5b09fd916c78",
   "metadata": {},
   "source": [
    "## Build a Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f4c784-d600-4b17-b494-3e98ee07af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classification model\n",
    "def classification_model():\n",
    "    # create model\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(num_pixels,)))\n",
    "    model.add(Dense(num_pixels, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d7bc7-1344-4d0d-857b-4a91c685c232",
   "metadata": {},
   "source": [
    "## Train and Test the Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "711b258f-73fe-4ace-ae96-e72c71d6af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 18s - 10ms/step - accuracy: 0.9427 - loss: 0.1889 - val_accuracy: 0.9726 - val_loss: 0.0867\n",
      "Epoch 2/10\n",
      "1875/1875 - 19s - 10ms/step - accuracy: 0.9753 - loss: 0.0782 - val_accuracy: 0.9712 - val_loss: 0.0886\n",
      "Epoch 3/10\n",
      "1875/1875 - 31s - 17ms/step - accuracy: 0.9829 - loss: 0.0542 - val_accuracy: 0.9779 - val_loss: 0.0736\n",
      "Epoch 4/10\n",
      "1875/1875 - 22s - 12ms/step - accuracy: 0.9867 - loss: 0.0408 - val_accuracy: 0.9784 - val_loss: 0.0746\n",
      "Epoch 5/10\n",
      "1875/1875 - 18s - 9ms/step - accuracy: 0.9895 - loss: 0.0321 - val_accuracy: 0.9792 - val_loss: 0.0751\n",
      "Epoch 6/10\n",
      "1875/1875 - 19s - 10ms/step - accuracy: 0.9915 - loss: 0.0259 - val_accuracy: 0.9798 - val_loss: 0.0781\n",
      "Epoch 7/10\n",
      "1875/1875 - 17s - 9ms/step - accuracy: 0.9925 - loss: 0.0228 - val_accuracy: 0.9786 - val_loss: 0.0834\n",
      "Epoch 8/10\n",
      "1875/1875 - 17s - 9ms/step - accuracy: 0.9934 - loss: 0.0197 - val_accuracy: 0.9827 - val_loss: 0.0750\n",
      "Epoch 9/10\n",
      "1875/1875 - 16s - 9ms/step - accuracy: 0.9941 - loss: 0.0177 - val_accuracy: 0.9785 - val_loss: 0.0893\n",
      "Epoch 10/10\n",
      "1875/1875 - 17s - 9ms/step - accuracy: 0.9950 - loss: 0.0164 - val_accuracy: 0.9804 - val_loss: 0.0827\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = classification_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8668581-4e3e-4cae-a0ba-1932c560aa01",
   "metadata": {},
   "source": [
    "Let's print the accuracy and the corresponding error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c62635ba-7b96-44d8-85e5-b93101313e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.04 % \n",
      " Error: 0.02\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2f} % \\n Error: {:.2f}'.format(scores[1]*100, 1 - scores[1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738fbd92-dfb1-49c5-b8d4-a1c0a60a4b06",
   "metadata": {},
   "source": [
    "Sometimes, we cannot afford to retrain your model everytime you want to use it, especially if you are limited on computational resources and training your model can take a long time. Therefore, with the Keras library, you can save your model after training. To do that, we use the save method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f2fd96-8a5e-4323-9fa2-0102ad9cabab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classification_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9a939-77b2-47df-ab3c-fb98d5951f3d",
   "metadata": {},
   "source": [
    "Since our model contains multidimensional arrays of data, then models are usually saved as .keras files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d23cda-d3df-451f-bb71-087284ef482d",
   "metadata": {},
   "source": [
    "When you are ready to use your model again, you use the load_model function from <strong>keras.saving</strong>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30487ebb-692c-4cdc-b3ea-ac0df17cfcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras.saving.load_model('classification_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4499a9-fdaa-4c5e-8a44-1332791440a0",
   "metadata": {},
   "source": [
    "Create a neural network model with 6 dense layers and compare its accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c920a54-1d95-4f5a-8f3a-95f1f933a775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">615,440</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">78,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">410</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │       \u001b[38;5;34m615,440\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m78,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m10,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │         \u001b[38;5;34m8,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │         \u001b[38;5;34m3,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m410\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">715,770</span> (2.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m715,770\u001b[0m (2.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">715,770</span> (2.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m715,770\u001b[0m (2.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Input(shape=(num_pixels,)))\n",
    "model2.add(Dense(num_pixels, activation='relu'))\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(Dense(80, activation='relu'))\n",
    "model2.add(Dense(40, activation='relu'))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 11s - 6ms/step - accuracy: 0.9325 - loss: 0.2213 - val_accuracy: 0.9681 - val_loss: 0.1020\n",
      "Epoch 2/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9716 - loss: 0.0966 - val_accuracy: 0.9727 - val_loss: 0.0907\n",
      "Epoch 3/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9798 - loss: 0.0685 - val_accuracy: 0.9711 - val_loss: 0.0999\n",
      "Epoch 4/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9835 - loss: 0.0551 - val_accuracy: 0.9779 - val_loss: 0.0847\n",
      "Epoch 5/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9863 - loss: 0.0449 - val_accuracy: 0.9756 - val_loss: 0.0890\n",
      "Epoch 6/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9887 - loss: 0.0367 - val_accuracy: 0.9785 - val_loss: 0.0837\n",
      "Epoch 7/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9906 - loss: 0.0314 - val_accuracy: 0.9760 - val_loss: 0.0887\n",
      "Epoch 8/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9915 - loss: 0.0299 - val_accuracy: 0.9803 - val_loss: 0.0809\n",
      "Epoch 9/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9929 - loss: 0.0254 - val_accuracy: 0.9749 - val_loss: 0.0999\n",
      "Epoch 10/10\n",
      "1875/1875 - 10s - 6ms/step - accuracy: 0.9933 - loss: 0.0227 - val_accuracy: 0.9801 - val_loss: 0.0871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1afd9b0ce20>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9760 - loss: 0.1009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.087115578353405, 0.9800999760627747]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('classification_model2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cca9e-a280-4083-9399-536639edcc00",
   "metadata": {},
   "source": [
    "Now, load the the earlier saved model, train it further for 10 more epochs and check the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02fdd930-699f-4483-842e-38f66775fd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 11s - 6ms/step - accuracy: 0.9933 - loss: 0.0208 - val_accuracy: 0.9794 - val_loss: 0.0958\n",
      "Epoch 2/10\n",
      "1875/1875 - 11s - 6ms/step - accuracy: 0.9941 - loss: 0.0195 - val_accuracy: 0.9815 - val_loss: 0.0852\n",
      "Epoch 3/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9949 - loss: 0.0182 - val_accuracy: 0.9831 - val_loss: 0.0833\n",
      "Epoch 4/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9957 - loss: 0.0174 - val_accuracy: 0.9833 - val_loss: 0.0788\n",
      "Epoch 5/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9956 - loss: 0.0157 - val_accuracy: 0.9806 - val_loss: 0.0897\n",
      "Epoch 6/10\n",
      "1875/1875 - 10s - 5ms/step - accuracy: 0.9969 - loss: 0.0117 - val_accuracy: 0.9777 - val_loss: 0.1206\n",
      "Epoch 7/10\n",
      "1875/1875 - 11s - 6ms/step - accuracy: 0.9954 - loss: 0.0161 - val_accuracy: 0.9818 - val_loss: 0.0924\n",
      "Epoch 8/10\n",
      "1875/1875 - 11s - 6ms/step - accuracy: 0.9963 - loss: 0.0130 - val_accuracy: 0.9828 - val_loss: 0.0892\n",
      "Epoch 9/10\n",
      "1875/1875 - 11s - 6ms/step - accuracy: 0.9962 - loss: 0.0135 - val_accuracy: 0.9810 - val_loss: 0.1170\n",
      "Epoch 10/10\n",
      "1875/1875 - 11s - 6ms/step - accuracy: 0.9970 - loss: 0.0110 - val_accuracy: 0.9838 - val_loss: 0.1058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1afd3c4e070>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = keras.saving.load_model('classification_model2.keras')\n",
    "\n",
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9774 - loss: 0.0970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08265936374664307, 0.980400025844574]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "prev_pub_hash": "c850b639d611b84ea2eecf5ddb84ea433849e20b6766a84b43a9df0c51e4e9ee"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
